{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8vri4p7Alx1"
   },
   "source": [
    "### Acknowledgements\n",
    "\n",
    "First, a huge thank you to [LucidRains](https://github.com/lucidrains) for his amazing work creating dalle2 model repo.\n",
    "\n",
    "Thank you to [Nousr](https://twitter.com/nousr_) and [Aidan](https://github.com/Veldrovive) for their work on the training scripts for the prior and decoder. As well, thank you to [Romain](https://github.com/rom1504) for his work on enabling multi-node training and for his part in creating the dataset.\n",
    "\n",
    "This was trained thanks to the generous donation of compute time from [LAION](https://laion.ai/) and its sponsors, especially [StabilityAI](https://stability.ai/) for the servers used in this project.\n",
    "This model was trained using the aesthetic subset of the [LAION2B dataset](https://laion.ai/blog/laion-5b/), details of the run can be found on [wandb](https://wandb.ai/nousr_laion/dalle2_train_decoder/reports/Decoder-Training--VmlldzoyMjEyMjcw)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHpA7GtYAlx3"
   },
   "source": [
    "#### This is a WIP notebook.\n",
    "It is missing the upsamplers so it can only produce 64x64 images and has only be trained for 0.5% of what OpenAI did for their DALLE2. It can still produce some impressive results, but is not nearly at the level you might see on the news yet.\n",
    "\n",
    "While the upsamplers have yet to be trained, we have substituted [SwinIR](https://github.com/JingyunLiang/SwinIR) to produce 256x256 images. This tends to flatten details, but still generally improves the quality of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZmRFKPkAlx3"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjnUmrV2Alx4"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "lPklyP0pg0mF"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ccbf9f09fd46899f1fbbd49b2d17b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Decoder:', index=4, options=('Original', 'New 1B (Aesthetic)', 'New 1.5B â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a097bea67954c1b9662fc0fb0942729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading decoder and decoder config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1087  100  1087    0     0   3240      0 --:--:-- --:--:-- --:--:--  3283\n",
      "\n",
      "  0 3400M    0 1837k    0     0  3137k      0  0:18:29 --:--:--  0:18:29 3137k\n",
      "  0 3400M    0 12.9M    0     0  8346k      0  0:06:57  0:00:01  0:06:56 11.1M\n",
      "  0 3400M    0 24.1M    0     0  9577k      0  0:06:03  0:00:02  0:06:01 11.1M\n",
      "  1 3400M    1 35.4M    0     0   9.8M      0  0:05:44  0:00:03  0:05:41 11.2M\n",
      "  1 3400M    1 46.6M    0     0  10.1M      0  0:05:34  0:00:04  0:05:30 11.2M\n",
      "  1 3400M    1 57.7M    0     0  10.3M      0  0:05:29  0:00:05  0:05:24 11.1M\n",
      "  2 3400M    2 68.9M    0     0  10.4M      0  0:05:24  0:00:06  0:05:18 11.2M\n",
      "  2 3400M    2 80.2M    0     0  10.5M      0  0:05:21  0:00:07  0:05:14 11.2M\n",
      "  2 3400M    2 91.4M    0     0  10.6M      0  0:05:19  0:00:08  0:05:11 11.2M\n",
      "  3 3400M    3  102M    0     0  10.7M      0  0:05:17  0:00:09  0:05:08 11.2M\n",
      "  3 3400M    3  112M    0     0  10.6M      0  0:05:20  0:00:10  0:05:10 10.9M\n",
      "  3 3400M    3  122M    0     0  10.6M      0  0:05:20  0:00:11  0:05:09 10.7M\n",
      "  3 3400M    3  134M    0     0  10.6M      0  0:05:19  0:00:12  0:05:07 10.7M\n",
      "  4 3400M    4  145M    0     0  10.6M      0  0:05:17  0:00:13  0:05:04 10.7M\n",
      "  4 3400M    4  156M    0     0  10.7M      0  0:05:16  0:00:14  0:05:02 10.7M\n",
      "  4 3400M    4  167M    0     0  10.7M      0  0:05:15  0:00:15  0:05:00 11.1M\n",
      "  5 3400M    5  179M    0     0  10.7M      0  0:05:15  0:00:16  0:04:59 11.2M\n",
      "  5 3400M    5  190M    0     0  10.8M      0  0:05:14  0:00:17  0:04:57 11.2M\n",
      "  5 3400M    5  201M    0     0  10.8M      0  0:05:13  0:00:18  0:04:55 11.2M\n",
      "  6 3400M    6  212M    0     0  10.8M      0  0:05:12  0:00:19  0:04:53 11.2M\n",
      "  6 3400M    6  224M    0     0  10.8M      0  0:05:12  0:00:20  0:04:52 11.2M\n",
      "  6 3400M    6  233M    0     0  10.8M      0  0:05:14  0:00:21  0:04:53 10.8M\n",
      "  7 3400M    7  244M    0     0  10.8M      0  0:05:14  0:00:22  0:04:52 10.8M\n",
      "  7 3400M    7  255M    0     0  10.8M      0  0:05:13  0:00:23  0:04:50 10.8M\n",
      "  7 3400M    7  265M    0     0  10.7M      0  0:05:15  0:00:24  0:04:51 10.4M\n",
      "  8 3400M    8  275M    0     0  10.7M      0  0:05:16  0:00:25  0:04:51 10.2M\n",
      "  8 3400M    8  286M    0     0  10.7M      0  0:05:15  0:00:26  0:04:49 10.6M\n",
      "  8 3400M    8  297M    0     0  10.7M      0  0:05:15  0:00:27  0:04:48 10.6M\n",
      "  8 3400M    8  304M    0     0  10.6M      0  0:05:18  0:00:28  0:04:50  9.8M\n",
      "  9 3400M    9  316M    0     0  10.6M      0  0:05:18  0:00:29  0:04:49 10.1M\n",
      "  9 3400M    9  327M    0     0  10.7M      0  0:05:17  0:00:30  0:04:47 10.4M\n",
      "  9 3400M    9  338M    0     0  10.7M      0  0:05:17  0:00:31  0:04:46 10.4M\n",
      " 10 3400M   10  349M    0     0  10.7M      0  0:05:16  0:00:32  0:04:44 10.4M\n",
      " 10 3400M   10  361M    0     0  10.7M      0  0:05:16  0:00:33  0:04:43 11.2M\n",
      " 10 3400M   10  372M    0     0  10.7M      0  0:05:15  0:00:34  0:04:41 11.2M\n",
      " 11 3400M   11  383M    0     0  10.7M      0  0:05:15  0:00:35  0:04:40 11.2M\n",
      " 11 3400M   11  394M    0     0  10.7M      0  0:05:15  0:00:36  0:04:39 11.2M\n",
      " 11 3400M   11  406M    0     0  10.8M      0  0:05:14  0:00:37  0:04:37 11.2M\n",
      " 12 3400M   12  417M    0     0  10.8M      0  0:05:14  0:00:38  0:04:36 11.2M\n",
      " 12 3400M   12  428M    0     0  10.8M      0  0:05:14  0:00:39  0:04:35 11.2M\n",
      " 12 3400M   12  439M    0     0  10.8M      0  0:05:13  0:00:40  0:04:33 11.2M\n",
      " 13 3400M   13  451M    0     0  10.8M      0  0:05:13  0:00:41  0:04:32 11.2M\n",
      " 13 3400M   13  461M    0     0  10.8M      0  0:05:14  0:00:42  0:04:32 10.9M\n",
      " 13 3400M   13  471M    0     0  10.8M      0  0:05:14  0:00:43  0:04:31 10.8M\n",
      " 14 3400M   14  482M    0     0  10.8M      0  0:05:14  0:00:44  0:04:30 10.8M\n",
      " 14 3400M   14  493M    0     0  10.8M      0  0:05:13  0:00:45  0:04:28 10.7M\n",
      " 14 3400M   14  505M    0     0  10.8M      0  0:05:13  0:00:46  0:04:27 10.7M\n",
      " 15 3400M   15  516M    0     0  10.8M      0  0:05:13  0:00:47  0:04:26 11.0M\n",
      " 15 3400M   15  527M    0     0  10.8M      0  0:05:13  0:00:48  0:04:25 11.2M\n",
      " 15 3400M   15  538M    0     0  10.8M      0  0:05:12  0:00:49  0:04:23 11.2M\n",
      " 16 3400M   16  550M    0     0  10.8M      0  0:05:12  0:00:50  0:04:22 11.2M\n",
      " 16 3400M   16  561M    0     0  10.8M      0  0:05:12  0:00:51  0:04:21 11.2M\n",
      " 16 3400M   16  572M    0     0  10.8M      0  0:05:12  0:00:52  0:04:20 11.2M\n",
      " 17 3400M   17  581M    0     0  10.8M      0  0:05:13  0:00:53  0:04:20 10.8M\n",
      " 17 3400M   17  592M    0     0  10.8M      0  0:05:13  0:00:54  0:04:19 10.8M\n",
      " 17 3400M   17  604M    0     0  10.8M      0  0:05:12  0:00:55  0:04:17 10.8M\n",
      " 18 3400M   18  615M    0     0  10.8M      0  0:05:12  0:00:56  0:04:16 10.8M\n",
      " 18 3400M   18  626M    0     0  10.8M      0  0:05:12  0:00:57  0:04:15 10.8M\n",
      " 18 3400M   18  637M    0     0  10.8M      0  0:05:12  0:00:58  0:04:14 11.2M\n",
      " 19 3400M   19  649M    0     0  10.8M      0  0:05:12  0:00:59  0:04:13 11.2M\n",
      " 19 3400M   19  660M    0     0  10.9M      0  0:05:11  0:01:00  0:04:11 11.2M\n",
      " 19 3400M   19  671M    0     0  10.9M      0  0:05:11  0:01:01  0:04:10 11.2M\n",
      " 20 3400M   20  682M    0     0  10.9M      0  0:05:11  0:01:02  0:04:09 11.2M\n",
      " 20 3400M   20  691M    0     0  10.8M      0  0:05:12  0:01:03  0:04:09 10.7M\n",
      " 20 3400M   20  703M    0     0  10.8M      0  0:05:12  0:01:04  0:04:08 10.7M\n",
      " 21 3400M   21  714M    0     0  10.8M      0  0:05:12  0:01:05  0:04:07 10.7M\n",
      " 21 3400M   21  725M    0     0  10.8M      0  0:05:12  0:01:06  0:04:06 10.7M\n",
      " 21 3400M   21  736M    0     0  10.9M      0  0:05:11  0:01:07  0:04:04 10.8M\n",
      " 21 3400M   21  748M    0     0  10.9M      0  0:05:11  0:01:08  0:04:03 11.2M\n",
      " 22 3400M   22  759M    0     0  10.9M      0  0:05:11  0:01:09  0:04:02 11.2M\n",
      " 22 3400M   22  770M    0     0  10.9M      0  0:05:11  0:01:10  0:04:01 11.2M\n",
      " 22 3400M   22  781M    0     0  10.9M      0  0:05:11  0:01:11  0:04:00 11.2M\n",
      " 23 3400M   23  792M    0     0  10.9M      0  0:05:11  0:01:12  0:03:59 11.1M\n",
      " 23 3400M   23  801M    0     0  10.8M      0  0:05:12  0:01:13  0:03:59 10.7M\n",
      " 23 3400M   23  813M    0     0  10.9M      0  0:05:11  0:01:14  0:03:57 10.7M\n",
      " 24 3400M   24  824M    0     0  10.9M      0  0:05:11  0:01:15  0:03:56 10.7M\n",
      " 24 3400M   24  835M    0     0  10.9M      0  0:05:11  0:01:16  0:03:55 10.7M\n",
      " 24 3400M   24  846M    0     0  10.9M      0  0:05:11  0:01:17  0:03:54 10.7M\n",
      " 25 3400M   25  857M    0     0  10.9M      0  0:05:11  0:01:18  0:03:53 11.1M\n",
      " 25 3400M   25  869M    0     0  10.9M      0  0:05:11  0:01:19  0:03:52 11.1M\n",
      " 25 3400M   25  880M    0     0  10.9M      0  0:05:11  0:01:20  0:03:51 11.1M\n",
      " 26 3400M   26  891M    0     0  10.9M      0  0:05:11  0:01:21  0:03:50 11.2M\n",
      " 26 3400M   26  902M    0     0  10.9M      0  0:05:11  0:01:22  0:03:49 11.2M\n",
      " 26 3400M   26  911M    0     0  10.9M      0  0:05:11  0:01:23  0:03:48 10.8M\n",
      " 27 3400M   27  923M    0     0  10.9M      0  0:05:11  0:01:24  0:03:47 10.8M\n",
      " 27 3400M   27  934M    0     0  10.9M      0  0:05:11  0:01:25  0:03:46 10.8M\n",
      " 27 3400M   27  945M    0     0  10.9M      0  0:05:11  0:01:26  0:03:45 10.8M\n",
      " 28 3400M   28  957M    0     0  10.9M      0  0:05:11  0:01:27  0:03:44 10.8M\n",
      " 28 3400M   28  968M    0     0  10.9M      0  0:05:11  0:01:28  0:03:43 11.2M\n",
      " 28 3400M   28  979M    0     0  10.9M      0  0:05:11  0:01:29  0:03:42 11.2M\n",
      " 29 3400M   29  990M    0     0  10.9M      0  0:05:10  0:01:30  0:03:40 11.2M\n",
      " 29 3400M   29 1001M    0     0  10.9M      0  0:05:10  0:01:31  0:03:39 11.2M\n",
      " 29 3400M   29 1013M    0     0  10.9M      0  0:05:10  0:01:32  0:03:38 11.2M\n",
      " 30 3400M   30 1024M    0     0  10.9M      0  0:05:10  0:01:33  0:03:37 11.2M\n",
      " 30 3400M   30 1035M    0     0  10.9M      0  0:05:10  0:01:34  0:03:36 11.2M\n",
      " 30 3400M   30 1046M    0     0  10.9M      0  0:05:10  0:01:35  0:03:35 11.2M\n",
      " 31 3400M   31 1058M    0     0  10.9M      0  0:05:10  0:01:36  0:03:34 11.2M\n",
      " 31 3400M   31 1069M    0     0  10.9M      0  0:05:10  0:01:37  0:03:33 11.2M\n",
      " 31 3400M   31 1080M    0     0  10.9M      0  0:05:10  0:01:38  0:03:32 11.2M\n",
      " 32 3400M   32 1089M    0     0  10.9M      0  0:05:10  0:01:39  0:03:31 10.8M\n",
      " 32 3400M   32 1100M    0     0  10.9M      0  0:05:10  0:01:40  0:03:30 10.8M\n",
      " 32 3400M   32 1112M    0     0  10.9M      0  0:05:10  0:01:41  0:03:29 10.8M\n",
      " 33 3400M   33 1123M    0     0  10.9M      0  0:05:10  0:01:42  0:03:28 10.8M\n",
      " 33 3400M   33 1134M    0     0  10.9M      0  0:05:10  0:01:43  0:03:27 10.8M\n",
      " 33 3400M   33 1145M    0     0  10.9M      0  0:05:10  0:01:44  0:03:26 11.2M\n",
      " 34 3400M   34 1156M    0     0  10.9M      0  0:05:10  0:01:45  0:03:25 11.2M\n",
      " 34 3400M   34 1168M    0     0  10.9M      0  0:05:10  0:01:46  0:03:24 11.2M\n",
      " 34 3400M   34 1179M    0     0  10.9M      0  0:05:10  0:01:47  0:03:23 11.2M\n",
      " 35 3400M   35 1190M    0     0  10.9M      0  0:05:10  0:01:48  0:03:22 11.1M\n",
      " 35 3400M   35 1201M    0     0  10.9M      0  0:05:10  0:01:49  0:03:21 11.2M\n",
      " 35 3400M   35 1210M    0     0  10.9M      0  0:05:10  0:01:50  0:03:20 10.7M\n",
      " 35 3400M   35 1222M    0     0  10.9M      0  0:05:10  0:01:51  0:03:19 10.7M\n",
      " 36 3400M   36 1233M    0     0  10.9M      0  0:05:10  0:01:52  0:03:18 10.7M\n",
      " 36 3400M   36 1244M    0     0  10.9M      0  0:05:10  0:01:53  0:03:17 10.8M\n",
      " 36 3400M   36 1255M    0     0  10.9M      0  0:05:10  0:01:54  0:03:16 10.8M\n",
      " 37 3400M   37 1267M    0     0  10.9M      0  0:05:10  0:01:55  0:03:15 11.2M\n",
      " 37 3400M   37 1278M    0     0  10.9M      0  0:05:10  0:01:56  0:03:14 11.2M\n",
      " 37 3400M   37 1289M    0     0  10.9M      0  0:05:10  0:01:57  0:03:13 11.2M\n",
      " 38 3400M   38 1300M    0     0  10.9M      0  0:05:10  0:01:58  0:03:12 11.2M\n",
      " 38 3400M   38 1311M    0     0  10.9M      0  0:05:09  0:01:59  0:03:10 11.2M\n",
      " 38 3400M   38 1321M    0     0  10.9M      0  0:05:10  0:02:00  0:03:10 10.7M\n",
      " 39 3400M   39 1332M    0     0  10.9M      0  0:05:10  0:02:01  0:03:09 10.7M\n",
      " 39 3400M   39 1343M    0     0  10.9M      0  0:05:10  0:02:02  0:03:08 10.7M\n",
      " 39 3400M   39 1354M    0     0  10.9M      0  0:05:10  0:02:03  0:03:07 10.8M\n",
      " 40 3400M   40 1365M    0     0  10.9M      0  0:05:10  0:02:04  0:03:06 10.7M\n",
      " 40 3400M   40 1376M    0     0  10.9M      0  0:05:10  0:02:05  0:03:05 11.1M\n",
      " 40 3400M   40 1387M    0     0  10.9M      0  0:05:10  0:02:06  0:03:04 11.1M\n",
      " 41 3400M   41 1399M    0     0  10.9M      0  0:05:10  0:02:07  0:03:03 11.1M\n",
      " 41 3400M   41 1410M    0     0  10.9M      0  0:05:10  0:02:08  0:03:02 11.1M\n",
      " 41 3400M   41 1421M    0     0  10.9M      0  0:05:10  0:02:09  0:03:01 11.1M\n",
      " 42 3400M   42 1430M    0     0  10.9M      0  0:05:10  0:02:10  0:03:00 10.7M\n",
      " 42 3400M   42 1441M    0     0  10.9M      0  0:05:10  0:02:11  0:02:59 10.7M\n",
      " 42 3400M   42 1452M    0     0  10.9M      0  0:05:10  0:02:12  0:02:58 10.7M\n",
      " 43 3400M   43 1464M    0     0  10.9M      0  0:05:10  0:02:13  0:02:57 10.7M\n",
      " 43 3400M   43 1475M    0     0  10.9M      0  0:05:10  0:02:14  0:02:56 10.7M\n",
      " 43 3400M   43 1486M    0     0  10.9M      0  0:05:10  0:02:15  0:02:55 11.2M\n",
      " 44 3400M   44 1497M    0     0  10.9M      0  0:05:10  0:02:16  0:02:54 11.2M\n",
      " 44 3400M   44 1509M    0     0  10.9M      0  0:05:10  0:02:17  0:02:53 11.2M\n",
      " 44 3400M   44 1520M    0     0  10.9M      0  0:05:10  0:02:18  0:02:52 11.2M\n",
      " 45 3400M   45 1531M    0     0  10.9M      0  0:05:09  0:02:19  0:02:50 11.2M\n",
      " 45 3400M   45 1540M    0     0  10.9M      0  0:05:10  0:02:20  0:02:50 10.7M\n",
      " 45 3400M   45 1551M    0     0  10.9M      0  0:05:10  0:02:21  0:02:49 10.7M\n",
      " 45 3400M   45 1562M    0     0  10.9M      0  0:05:10  0:02:22  0:02:48 10.7M\n",
      " 46 3400M   46 1574M    0     0  10.9M      0  0:05:10  0:02:23  0:02:47 10.7M\n",
      " 46 3400M   46 1585M    0     0  10.9M      0  0:05:10  0:02:24  0:02:46 10.7M\n",
      " 46 3400M   46 1596M    0     0  10.9M      0  0:05:10  0:02:25  0:02:45 11.2M\n",
      " 47 3400M   47 1607M    0     0  10.9M      0  0:05:10  0:02:26  0:02:44 11.0M\n",
      " 47 3400M   47 1616M    0     0  10.9M      0  0:05:10  0:02:27  0:02:43 10.6M\n",
      " 47 3400M   47 1627M    0     0  10.9M      0  0:05:10  0:02:28  0:02:42 10.6M\n",
      " 48 3400M   48 1635M    0     0  10.9M      0  0:05:11  0:02:29  0:02:42 10.0M\n",
      " 48 3400M   48 1646M    0     0  10.9M      0  0:05:11  0:02:30  0:02:41  9.9M\n",
      " 48 3400M   48 1657M    0     0  10.9M      0  0:05:10  0:02:31  0:02:39 10.1M\n",
      " 49 3400M   49 1669M    0     0  10.9M      0  0:05:10  0:02:32  0:02:38 10.5M\n",
      " 49 3400M   49 1680M    0     0  10.9M      0  0:05:10  0:02:33  0:02:37 10.5M\n",
      " 49 3400M   49 1691M    0     0  10.9M      0  0:05:10  0:02:34  0:02:36 11.2M\n",
      " 50 3400M   50 1702M    0     0  10.9M      0  0:05:10  0:02:35  0:02:35 11.2M\n",
      " 50 3400M   50 1713M    0     0  10.9M      0  0:05:10  0:02:36  0:02:34 11.2M\n",
      " 50 3400M   50 1725M    0     0  10.9M      0  0:05:10  0:02:37  0:02:33 11.2M\n",
      " 51 3400M   51 1736M    0     0  10.9M      0  0:05:10  0:02:38  0:02:32 11.1M\n",
      " 51 3400M   51 1746M    0     0  10.9M      0  0:05:10  0:02:39  0:02:31 10.8M\n",
      " 51 3400M   51 1756M    0     0  10.9M      0  0:05:10  0:02:40  0:02:30 10.7M\n",
      " 51 3400M   51 1767M    0     0  10.9M      0  0:05:10  0:02:41  0:02:29 10.7M\n",
      " 52 3400M   52 1778M    0     0  10.9M      0  0:05:10  0:02:42  0:02:28 10.7M\n",
      " 52 3400M   52 1789M    0     0  10.9M      0  0:05:10  0:02:43  0:02:27 10.7M\n",
      " 52 3400M   52 1801M    0     0  10.9M      0  0:05:10  0:02:44  0:02:26 11.0M\n",
      " 53 3400M   53 1812M    0     0  10.9M      0  0:05:10  0:02:45  0:02:25 11.1M\n",
      " 53 3400M   53 1823M    0     0  10.9M      0  0:05:10  0:02:46  0:02:24 11.2M\n",
      " 53 3400M   53 1835M    0     0  10.9M      0  0:05:10  0:02:47  0:02:23 11.2M\n",
      " 54 3400M   54 1846M    0     0  10.9M      0  0:05:10  0:02:48  0:02:22 11.2M\n",
      " 54 3400M   54 1857M    0     0  10.9M      0  0:05:10  0:02:49  0:02:21 11.2M\n",
      " 54 3400M   54 1866M    0     0  10.9M      0  0:05:10  0:02:50  0:02:20 10.8M\n",
      " 55 3400M   55 1877M    0     0  10.9M      0  0:05:10  0:02:51  0:02:19 10.8M\n",
      " 55 3400M   55 1889M    0     0  10.9M      0  0:05:10  0:02:52  0:02:18 10.8M\n",
      " 55 3400M   55 1900M    0     0  10.9M      0  0:05:10  0:02:53  0:02:17 10.8M\n",
      " 56 3400M   56 1911M    0     0  10.9M      0  0:05:10  0:02:54  0:02:16 10.8M\n",
      " 56 3400M   56 1922M    0     0  10.9M      0  0:05:10  0:02:55  0:02:15 11.2M\n",
      " 56 3400M   56 1933M    0     0  10.9M      0  0:05:10  0:02:56  0:02:14 11.0M\n",
      " 57 3400M   57 1944M    0     0  10.9M      0  0:05:10  0:02:57  0:02:13 11.0M\n",
      " 57 3400M   57 1955M    0     0  10.9M      0  0:05:10  0:02:58  0:02:12 11.0M\n",
      " 57 3400M   57 1966M    0     0  10.9M      0  0:05:10  0:02:59  0:02:11 11.0M\n",
      " 58 3400M   58 1975M    0     0  10.9M      0  0:05:10  0:03:00  0:02:10 10.6M\n",
      " 58 3400M   58 1986M    0     0  10.9M      0  0:05:10  0:03:01  0:02:09 10.7M\n",
      " 58 3400M   58 1998M    0     0  10.9M      0  0:05:10  0:03:02  0:02:08 10.7M\n",
      " 59 3400M   59 2009M    0     0  10.9M      0  0:05:10  0:03:03  0:02:07 10.8M\n",
      " 59 3400M   59 2020M    0     0  10.9M      0  0:05:10  0:03:04  0:02:06 10.8M\n",
      " 59 3400M   59 2032M    0     0  10.9M      0  0:05:10  0:03:05  0:02:05 11.2M\n",
      " 60 3400M   60 2043M    0     0  10.9M      0  0:05:10  0:03:06  0:02:04 11.2M\n",
      " 60 3400M   60 2054M    0     0  10.9M      0  0:05:10  0:03:07  0:02:03 11.2M\n",
      " 60 3400M   60 2065M    0     0  10.9M      0  0:05:10  0:03:08  0:02:02 11.2M\n",
      " 61 3400M   61 2076M    0     0  10.9M      0  0:05:10  0:03:09  0:02:01 11.2M\n",
      " 61 3400M   61 2086M    0     0  10.9M      0  0:05:10  0:03:10  0:02:00 10.8M\n",
      " 61 3400M   61 2097M    0     0  10.9M      0  0:05:10  0:03:11  0:01:59 10.7M\n",
      " 62 3400M   62 2108M    0     0  10.9M      0  0:05:10  0:03:12  0:01:58 10.8M\n",
      " 62 3400M   62 2119M    0     0  10.9M      0  0:05:10  0:03:13  0:01:57 10.8M\n",
      " 62 3400M   62 2131M    0     0  10.9M      0  0:05:10  0:03:14  0:01:56 10.8M\n",
      " 62 3400M   62 2142M    0     0  10.9M      0  0:05:10  0:03:15  0:01:55 11.2M\n",
      " 63 3400M   63 2153M    0     0  10.9M      0  0:05:10  0:03:16  0:01:54 11.2M\n",
      " 63 3400M   63 2164M    0     0  10.9M      0  0:05:10  0:03:17  0:01:53 11.2M\n",
      " 63 3400M   63 2175M    0     0  10.9M      0  0:05:10  0:03:18  0:01:52 11.2M\n",
      " 64 3400M   64 2187M    0     0  10.9M      0  0:05:10  0:03:19  0:01:51 11.2M\n",
      " 64 3400M   64 2196M    0     0  10.9M      0  0:05:10  0:03:20  0:01:50 10.8M\n",
      " 64 3400M   64 2207M    0     0  10.9M      0  0:05:10  0:03:21  0:01:49 10.8M\n",
      " 65 3400M   65 2218M    0     0  10.9M      0  0:05:10  0:03:22  0:01:48 10.8M\n",
      " 65 3400M   65 2230M    0     0  10.9M      0  0:05:10  0:03:23  0:01:47 10.8M\n",
      " 65 3400M   65 2241M    0     0  10.9M      0  0:05:10  0:03:24  0:01:46 10.8M\n",
      " 66 3400M   66 2252M    0     0  10.9M      0  0:05:10  0:03:25  0:01:45 11.1M\n",
      " 66 3400M   66 2263M    0     0  10.9M      0  0:05:10  0:03:26  0:01:44 11.2M\n",
      " 66 3400M   66 2275M    0     0  10.9M      0  0:05:10  0:03:27  0:01:43 11.2M\n",
      " 67 3400M   67 2286M    0     0  10.9M      0  0:05:10  0:03:28  0:01:42 11.2M\n",
      " 67 3400M   67 2297M    0     0  10.9M      0  0:05:10  0:03:29  0:01:41 11.2M\n",
      " 67 3400M   67 2308M    0     0  10.9M      0  0:05:10  0:03:30  0:01:40 11.2M\n",
      " 68 3400M   68 2318M    0     0  10.9M      0  0:05:10  0:03:31  0:01:39 10.8M\n",
      " 68 3400M   68 2329M    0     0  10.9M      0  0:05:10  0:03:32  0:01:38 10.8M\n",
      " 68 3400M   68 2340M    0     0  10.9M      0  0:05:10  0:03:33  0:01:37 10.8M\n",
      " 69 3400M   69 2351M    0     0  10.9M      0  0:05:10  0:03:34  0:01:36 10.8M\n",
      " 69 3400M   69 2363M    0     0  10.9M      0  0:05:10  0:03:35  0:01:35 10.8M\n",
      " 69 3400M   69 2374M    0     0  10.9M      0  0:05:10  0:03:36  0:01:34 11.2M\n",
      " 70 3400M   70 2385M    0     0  10.9M      0  0:05:10  0:03:37  0:01:33 11.2M\n",
      " 70 3400M   70 2396M    0     0  10.9M      0  0:05:10  0:03:38  0:01:32 11.2M\n",
      " 70 3400M   70 2407M    0     0  10.9M      0  0:05:10  0:03:39  0:01:31 11.2M\n",
      " 71 3400M   71 2419M    0     0  10.9M      0  0:05:10  0:03:40  0:01:30 11.2M\n",
      " 71 3400M   71 2428M    0     0  10.9M      0  0:05:10  0:03:41  0:01:29 10.8M\n",
      " 71 3400M   71 2439M    0     0  10.9M      0  0:05:10  0:03:42  0:01:28 10.8M\n",
      " 72 3400M   72 2450M    0     0  10.9M      0  0:05:10  0:03:43  0:01:27 10.8M\n",
      " 72 3400M   72 2462M    0     0  10.9M      0  0:05:10  0:03:44  0:01:26 10.8M\n",
      " 72 3400M   72 2473M    0     0  10.9M      0  0:05:10  0:03:45  0:01:25 10.8M\n",
      " 73 3400M   73 2484M    0     0  10.9M      0  0:05:10  0:03:46  0:01:24 11.2M\n",
      " 73 3400M   73 2495M    0     0  10.9M      0  0:05:10  0:03:47  0:01:23 11.2M\n",
      " 73 3400M   73 2507M    0     0  10.9M      0  0:05:10  0:03:48  0:01:22 11.2M\n",
      " 74 3400M   74 2518M    0     0  10.9M      0  0:05:10  0:03:49  0:01:21 11.2M\n",
      " 74 3400M   74 2529M    0     0  10.9M      0  0:05:09  0:03:50  0:01:19 11.2M\n",
      " 74 3400M   74 2538M    0     0  10.9M      0  0:05:10  0:03:51  0:01:19 10.8M\n",
      " 74 3400M   74 2550M    0     0  10.9M      0  0:05:10  0:03:52  0:01:18 10.8M\n",
      " 75 3400M   75 2561M    0     0  10.9M      0  0:05:10  0:03:53  0:01:17 10.8M\n",
      " 75 3400M   75 2572M    0     0  10.9M      0  0:05:10  0:03:54  0:01:16 10.8M\n",
      " 75 3400M   75 2583M    0     0  10.9M      0  0:05:10  0:03:55  0:01:15 10.8M\n",
      " 76 3400M   76 2595M    0     0  10.9M      0  0:05:10  0:03:56  0:01:14 11.2M\n",
      " 76 3400M   76 2606M    0     0  10.9M      0  0:05:09  0:03:57  0:01:12 11.2M\n",
      " 76 3400M   76 2617M    0     0  10.9M      0  0:05:09  0:03:58  0:01:11 11.2M\n",
      " 77 3400M   77 2628M    0     0  10.9M      0  0:05:09  0:03:59  0:01:10 11.2M\n",
      " 77 3400M   77 2640M    0     0  10.9M      0  0:05:09  0:04:00  0:01:09 11.2M\n",
      " 77 3400M   77 2649M    0     0  10.9M      0  0:05:10  0:04:01  0:01:09 10.7M\n",
      " 78 3400M   78 2660M    0     0  10.9M      0  0:05:10  0:04:02  0:01:08 10.7M\n",
      " 78 3400M   78 2671M    0     0  10.9M      0  0:05:10  0:04:03  0:01:07 10.7M\n",
      " 78 3400M   78 2682M    0     0  10.9M      0  0:05:10  0:04:04  0:01:06 10.7M\n",
      " 79 3400M   79 2693M    0     0  10.9M      0  0:05:10  0:04:05  0:01:05 10.7M\n",
      " 79 3400M   79 2705M    0     0  10.9M      0  0:05:09  0:04:06  0:01:03 11.2M\n",
      " 79 3400M   79 2716M    0     0  10.9M      0  0:05:09  0:04:07  0:01:02 11.2M\n",
      " 80 3400M   80 2727M    0     0  10.9M      0  0:05:09  0:04:08  0:01:01 11.2M\n",
      " 80 3400M   80 2738M    0     0  10.9M      0  0:05:09  0:04:09  0:01:00 11.2M\n",
      " 80 3400M   80 2750M    0     0  10.9M      0  0:05:09  0:04:10  0:00:59 11.2M\n",
      " 81 3400M   81 2761M    0     0  10.9M      0  0:05:09  0:04:11  0:00:58 11.2M\n",
      " 81 3400M   81 2772M    0     0  10.9M      0  0:05:09  0:04:12  0:00:57 11.2M\n",
      " 81 3400M   81 2783M    0     0  10.9M      0  0:05:09  0:04:13  0:00:56 11.1M\n",
      " 82 3400M   82 2795M    0     0  10.9M      0  0:05:09  0:04:14  0:00:55 11.2M\n",
      " 82 3400M   82 2806M    0     0  10.9M      0  0:05:09  0:04:15  0:00:54 11.2M\n",
      " 82 3400M   82 2815M    0     0  10.9M      0  0:05:09  0:04:16  0:00:53 10.7M\n",
      " 83 3400M   83 2826M    0     0  10.9M      0  0:05:09  0:04:17  0:00:52 10.7M\n",
      " 83 3400M   83 2837M    0     0  10.9M      0  0:05:09  0:04:18  0:00:51 10.8M\n",
      " 83 3400M   83 2849M    0     0  10.9M      0  0:05:09  0:04:19  0:00:50 10.8M\n",
      " 84 3400M   84 2860M    0     0  10.9M      0  0:05:09  0:04:20  0:00:49 10.8M\n",
      " 84 3400M   84 2871M    0     0  10.9M      0  0:05:09  0:04:21  0:00:48 11.2M\n",
      " 84 3400M   84 2882M    0     0  10.9M      0  0:05:09  0:04:22  0:00:47 11.2M\n",
      " 85 3400M   85 2894M    0     0  10.9M      0  0:05:09  0:04:23  0:00:46 11.2M\n",
      " 85 3400M   85 2905M    0     0  10.9M      0  0:05:09  0:04:24  0:00:45 11.2M\n",
      " 85 3400M   85 2916M    0     0  10.9M      0  0:05:09  0:04:25  0:00:44 11.2M\n",
      " 86 3400M   86 2926M    0     0  10.9M      0  0:05:09  0:04:26  0:00:43 10.8M\n",
      " 86 3400M   86 2937M    0     0  10.9M      0  0:05:09  0:04:27  0:00:42 10.8M\n",
      " 86 3400M   86 2948M    0     0  10.9M      0  0:05:09  0:04:28  0:00:41 10.7M\n",
      " 87 3400M   87 2959M    0     0  10.9M      0  0:05:09  0:04:29  0:00:40 10.7M\n",
      " 87 3400M   87 2970M    0     0  10.9M      0  0:05:09  0:04:30  0:00:39 10.8M\n",
      " 87 3400M   87 2981M    0     0  10.9M      0  0:05:09  0:04:31  0:00:38 11.1M\n",
      " 88 3400M   88 2993M    0     0  10.9M      0  0:05:09  0:04:32  0:00:37 11.2M\n",
      " 88 3400M   88 3004M    0     0  10.9M      0  0:05:09  0:04:33  0:00:36 11.2M\n",
      " 88 3400M   88 3015M    0     0  10.9M      0  0:05:09  0:04:34  0:00:35 11.2M\n",
      " 89 3400M   89 3026M    0     0  10.9M      0  0:05:09  0:04:35  0:00:34 11.2M\n",
      " 89 3400M   89 3038M    0     0  10.9M      0  0:05:09  0:04:36  0:00:33 11.2M\n",
      " 89 3400M   89 3047M    0     0  10.9M      0  0:05:09  0:04:37  0:00:32 10.8M\n",
      " 89 3400M   89 3058M    0     0  10.9M      0  0:05:09  0:04:38  0:00:31 10.8M\n",
      " 90 3400M   90 3069M    0     0  10.9M      0  0:05:09  0:04:39  0:00:30 10.8M\n",
      " 90 3400M   90 3080M    0     0  10.9M      0  0:05:09  0:04:40  0:00:29 10.7M\n",
      " 90 3400M   90 3092M    0     0  10.9M      0  0:05:09  0:04:41  0:00:28 10.7M\n",
      " 91 3400M   91 3103M    0     0  10.9M      0  0:05:09  0:04:42  0:00:27 11.1M\n",
      " 91 3400M   91 3114M    0     0  10.9M      0  0:05:09  0:04:43  0:00:26 11.1M\n",
      " 91 3400M   91 3125M    0     0  10.9M      0  0:05:09  0:04:44  0:00:25 11.1M\n",
      " 92 3400M   92 3136M    0     0  10.9M      0  0:05:09  0:04:45  0:00:24 11.2M\n",
      " 92 3400M   92 3148M    0     0  10.9M      0  0:05:09  0:04:46  0:00:23 11.2M\n",
      " 92 3400M   92 3157M    0     0  10.9M      0  0:05:09  0:04:47  0:00:22 10.7M\n",
      " 93 3400M   93 3168M    0     0  10.9M      0  0:05:09  0:04:48  0:00:21 10.7M\n",
      " 93 3400M   93 3179M    0     0  10.9M      0  0:05:09  0:04:49  0:00:20 10.7M\n",
      " 93 3400M   93 3190M    0     0  10.9M      0  0:05:09  0:04:50  0:00:19 10.7M\n",
      " 94 3400M   94 3201M    0     0  10.9M      0  0:05:09  0:04:51  0:00:18 10.7M\n",
      " 94 3400M   94 3213M    0     0  10.9M      0  0:05:09  0:04:52  0:00:17 11.1M\n",
      " 94 3400M   94 3224M    0     0  10.9M      0  0:05:09  0:04:53  0:00:16 11.1M\n",
      " 95 3400M   95 3235M    0     0  10.9M      0  0:05:09  0:04:54  0:00:15 11.1M\n",
      " 95 3400M   95 3246M    0     0  10.9M      0  0:05:09  0:04:55  0:00:14 11.2M\n",
      " 95 3400M   95 3258M    0     0  10.9M      0  0:05:09  0:04:56  0:00:13 11.2M\n",
      " 96 3400M   96 3267M    0     0  10.9M      0  0:05:09  0:04:57  0:00:12 10.8M\n",
      " 96 3400M   96 3278M    0     0  10.9M      0  0:05:09  0:04:58  0:00:11 10.8M\n",
      " 96 3400M   96 3289M    0     0  10.9M      0  0:05:09  0:04:59  0:00:10 10.8M\n",
      " 97 3400M   97 3300M    0     0  10.9M      0  0:05:09  0:05:00  0:00:09 10.8M\n",
      " 97 3400M   97 3311M    0     0  10.9M      0  0:05:09  0:05:01  0:00:08 10.7M\n",
      " 97 3400M   97 3322M    0     0  10.9M      0  0:05:09  0:05:02  0:00:07 11.1M\n",
      " 98 3400M   98 3334M    0     0  10.9M      0  0:05:09  0:05:03  0:00:06 11.1M\n",
      " 98 3400M   98 3345M    0     0  10.9M      0  0:05:09  0:05:04  0:00:05 11.1M\n",
      " 98 3400M   98 3356M    0     0  10.9M      0  0:05:09  0:05:05  0:00:04 11.1M\n",
      " 99 3400M   99 3367M    0     0  10.9M      0  0:05:09  0:05:06  0:00:03 11.2M\n",
      " 99 3400M   99 3376M    0     0  10.9M      0  0:05:09  0:05:07  0:00:02 10.7M\n",
      " 99 3400M   99 3387M    0     0  10.9M      0  0:05:09  0:05:08  0:00:01 10.7M\n",
      " 99 3400M   99 3399M    0     0  10.9M      0  0:05:09  0:05:09 --:--:-- 10.7M\n",
      "100 3400M  100 3400M    0     0  10.9M      0  0:05:09  0:05:09 --:--:-- 10.6M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading prior and prior config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  3186  100  3186    0     0  11273      0 --:--:-- --:--:-- --:--:-- 11419\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "!pip install -q ipywidgets\n",
    "import ipywidgets as widgets\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "decoder_versions = [{\n",
    "    \"name\": \"Original\",\n",
    "    \"dalle2_install_path\": \"git+https://github.com/Veldrovive/DALLE2-pytorch@f4b687798d367fc434d8127ab31141f0fea0db26\",\n",
    "    \"decoder_path\": \"https://huggingface.co/Veldrovive/DA-VINC-E/resolve/main/text_conditioned_epoch_34.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/Veldrovive/DA-VINC-E/raw/main/text_conditioned_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1B (Aesthetic)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/small_32gpus/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/small_32gpus/decoder_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1.5B (Aesthetic)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/1.5B/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/1.5B/decoder_config.json\"\n",
    "},{\n",
    "    \"name\": \"New 1.5B (Laion2B)\",\n",
    "    \"dalle2_install_path\": \"dalle2_pytorch==0.15.4\",\n",
    "    \"decoder_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/1.5B_laion2B/latest.pth\",\n",
    "    \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/1.5B_laion2B/decoder_config.json\"\n",
    "},{\n",
    "    \"name\": \"Upsampler\",\n",
    "    \"dalle2_install_path\": \"git+https://github.com/Veldrovive/DALLE2-pytorch@b2549a4d17244dab09e7a9496a9cb6330b7d3070\",\n",
    "    \"decoder\": [\n",
    "        {\n",
    "            \"unets\": [0],\n",
    "            \"model_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/resolve/main/decoder/1.5B_laion2B/latest.pth\",\n",
    "            \"config_path\": \"https://huggingface.co/laion/DALLE2-PyTorch/raw/main/decoder/1.5B_laion2B/decoder_config.json\"\n",
    "        },\n",
    "        {\n",
    "            \"unets\": [1],\n",
    "            \"model_path\": \"https://huggingface.co/Veldrovive/upsamplers/resolve/main/working/latest.pth\",\n",
    "            \"config_path\": \"https://huggingface.co/Veldrovive/upsamplers/raw/main/working/decoder_config.json\"\n",
    "        }\n",
    "    ],\n",
    "    \"prior\": {\n",
    "        \"model_path\": \"https://huggingface.co/zenglishuci/conditioned-prior/resolve/main/vit-l-14/prior_aes_finetune.pth\",\n",
    "        \"config_path\": \"\"\n",
    "    }\n",
    "}]\n",
    "\n",
    "decoder_options = [version[\"name\"] for version in decoder_versions]\n",
    "\n",
    "def load_state():\n",
    "    state_path = \"script_state.json\"\n",
    "    try:\n",
    "        assert os.path.exists(state_path)\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        # Make sure the save config is up to date. You might think this is a stupid system but...\n",
    "        decoder = state[\"decoder\"]\n",
    "        if decoder is not None:\n",
    "          current_decoder_name = decoder[\"name\"]\n",
    "          try:\n",
    "              current_decoder_index = decoder_options.index(current_decoder_name)\n",
    "              state[\"decoder\"] = decoder_versions[current_decoder_index]\n",
    "          except ValueError:\n",
    "              print(\"The decoder you were using no longer exists. Please pick a new option.\")\n",
    "              state[\"decoder\"] = None\n",
    "        \n",
    "        # Check if models are where they say they are\n",
    "        for filekey in [\"decoder\", \"decoder_config\", \"prior\", \"prior_config\"]:\n",
    "            path = state[\"model_paths\"][filekey]\n",
    "            if path is not None and not os.path.exists(path):\n",
    "                print(f\"{filekey} not found in expected place. Removing decoder config.\")\n",
    "                state[\"decoder\"] = None\n",
    "                state[\"model_paths\"] = {\n",
    "                    \"decoder\": None,\n",
    "                    \"decoder_config\": None,\n",
    "                    \"prior\": None,\n",
    "                    \"prior_config\": None\n",
    "                }\n",
    "                save_state()\n",
    "    except Exception as e:\n",
    "        state = {\n",
    "            \"text_input\": '',\n",
    "            \"text_repeat\": 3,\n",
    "            \"prior_conditioning\": 1.0,\n",
    "            \"img_repeat\": 1,\n",
    "            \"decoder_conditioning\": 3.5,\n",
    "            \"include_prompt_checkbox\": True,\n",
    "            \"upsample_checkbox\": True,\n",
    "            \"decoder\": None,\n",
    "            \"model_paths\": {\n",
    "                \"decoder\": None,\n",
    "                \"decoder_config\": None,\n",
    "                \"prior\": None,\n",
    "                \"prior_config\": None\n",
    "            }\n",
    "        }\n",
    "    return state\n",
    "\n",
    "current_state = load_state()\n",
    "\n",
    "def save_state():\n",
    "    global current_state\n",
    "    state_path = \"script_state.json\"\n",
    "    with open(state_path, \"w\") as f:\n",
    "        json.dump(current_state, f)\n",
    "\n",
    "def choice_equal(new_choice):\n",
    "    global current_state\n",
    "    if current_state[\"decoder\"] is None:\n",
    "        return False\n",
    "    return current_state[\"decoder\"][\"decoder_path\"] == new_choice[\"decoder_path\"]\n",
    "\n",
    "def dalle2_imported():\n",
    "    return \"dalle2_pytorch\" in sys.modules\n",
    "\n",
    "chosen_decoder = current_state[\"decoder\"] if current_state[\"decoder\"] is not None else decoder_versions[-1]\n",
    "\n",
    "decoder_version_dropdown = widgets.Dropdown(\n",
    "    options=decoder_options,\n",
    "    value=chosen_decoder[\"name\"],\n",
    "    description='Decoder:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "start_setup_button = widgets.Button(\n",
    "    description=\"Setup\"\n",
    ")\n",
    "\n",
    "redownload_button = widgets.Button (\n",
    "    description=\"Force Update Models\"\n",
    ")\n",
    "\n",
    "main_layout = widgets.VBox([decoder_version_dropdown, start_setup_button, redownload_button])\n",
    "\n",
    "def setup(decoder_version_name):\n",
    "    global current_state\n",
    "    global chosen_decoder\n",
    "    new_choice = decoder_versions[decoder_options.index(decoder_version_name)]\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    \n",
    "    new_choice_equal = choice_equal(new_choice)\n",
    "    already_imported = dalle2_imported()\n",
    "    \n",
    "    requires_restart = not new_choice_equal and already_imported  # The wrong dalle2_pytorch version is already imported\n",
    "    requires_download = not new_choice_equal  # The wrong decoder version is downloaded\n",
    "    \n",
    "    print(f\"You are using the model {new_choice['name']} which will be downloaded from {new_choice['decoder_path']}\\n\")\n",
    "    if requires_restart:\n",
    "        print(\"You environment already has dalle2 imported and collab requires a restart for you to be able to import the new version.\")\n",
    "        print(\"Restart your runtime to proceed.\")\n",
    "    elif requires_download:\n",
    "        print(\"The models are not downloaded. They will be when you proceed.\")\n",
    "    else:\n",
    "        print(\"You are ready to run inference. If you suspect your models are out of date, force update them.\")\n",
    "    \n",
    "    chosen_decoder = new_choice\n",
    "    \n",
    "\n",
    "out = widgets.interactive_output(setup, { 'decoder_version_name': decoder_version_dropdown })\n",
    "display(main_layout, out)\n",
    "\n",
    "def download_models(current_choice):\n",
    "    model_dir = \"./models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Download decoder\n",
    "    print(\"Downloading decoder and decoder config\")\n",
    "    decoder_url = current_choice[\"decoder_path\"]\n",
    "    decoder_config_url = current_choice[\"config_path\"]\n",
    "\n",
    "    decoder_path = os.path.join(model_dir, \"decoder.pth\")\n",
    "    decoder_config_path = os.path.join(model_dir, \"decoder_config.json\")\n",
    "    \n",
    "    !curl -L {decoder_url} > {decoder_path}\n",
    "    !curl -L {decoder_config_url} > {decoder_config_path}\n",
    "    \n",
    "    # Download prior\n",
    "    print(\"Downloading prior and prior config\")\n",
    "    prior_url = \"https://huggingface.co/zenglishuci/conditioned-prior/resolve/main/vit-l-14/prior_aes_finetune.pth\"\n",
    "        \n",
    "    prior_path = os.path.join(model_dir, \"prior.pth\")\n",
    "    \n",
    "    !curl -L {prior_url} > {prior_path}\n",
    "    return decoder_path, decoder_config_path, prior_path, None\n",
    "\n",
    "def install_dependencies(state):\n",
    "    print(\"Installing dependencies\")\n",
    "    dalle2_install_path = state[\"decoder\"][\"dalle2_install_path\"]\n",
    "    print(f\"Installing dalle2 version {dalle2_install_path}\")\n",
    "    !pip install -q {dalle2_install_path}\n",
    "    \n",
    "    !pip install -q matplotlib\n",
    "\n",
    "    print(\"Do not worry if you get the error `fatal: destination path 'SwinIR' already exists and is not an empty directory.`\")\n",
    "    print(\"That just means SwinIR is already installed and I'm too lazy to do the check myself\")\n",
    "    !git clone https://github.com/JingyunLiang/SwinIR.git\n",
    "    !pip install -q timm\n",
    "    !pip install -q opencv-python\n",
    "\n",
    "def start_setup(b):\n",
    "    global current_state\n",
    "    global chosen_decoder\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    \n",
    "    new_choice_equal = choice_equal(chosen_decoder)\n",
    "    already_imported = dalle2_imported()\n",
    "    \n",
    "    requires_restart = not new_choice_equal and already_imported  # The wrong dalle2_pytorch version is already imported\n",
    "    requires_download = not new_choice_equal  # The wrong decoder version is downloaded\n",
    "    \n",
    "    if requires_restart:\n",
    "        raise Exception(\"Please restart your runtime before trying to set up the environment\")\n",
    "        \n",
    "    if requires_download:\n",
    "        try:\n",
    "            decoder_path, decoder_config_path, prior_path, prior_config_path = download_models(chosen_decoder)\n",
    "        except Exception as e:\n",
    "            print(\"Model download was interrupted. Manually delete all models or environment may be corrupted.\")\n",
    "            current_state[\"decoder\"] = None\n",
    "            save_state()\n",
    "            raise e\n",
    "        current_state[\"decoder\"] = chosen_decoder\n",
    "        current_state[\"model_paths\"] = {\n",
    "                \"decoder\": decoder_path,\n",
    "                \"decoder_config\": decoder_config_path,\n",
    "                \"prior\": prior_path,\n",
    "                \"prior_config\": prior_config_path\n",
    "            }\n",
    "        save_state()\n",
    "    \n",
    "    install_dependencies(current_state)\n",
    "\n",
    "start_setup_button.on_click(start_setup)\n",
    "\n",
    "def force_download(b):\n",
    "    global current_state\n",
    "    current_choice = current_state[\"decoder\"]\n",
    "    updated_choice = decoder_versions[decoder_options.index(current_choice[\"name\"])]\n",
    "    chosen_decoder = updated_choice\n",
    "    try:\n",
    "        decoder_path, decoder_config_path, prior_path, prior_config_path = download_models(chosen_decoder)\n",
    "    except Exception as e:\n",
    "        print(\"Model download was interrupted. Force update models or environment may be corrupted.\")\n",
    "        current_state[\"decoder\"] = None\n",
    "        save_state()\n",
    "        raise e\n",
    "    current_state[\"decoder\"] = chosen_decoder\n",
    "    current_state[\"model_paths\"] = {\n",
    "            \"decoder\": decoder_path,\n",
    "            \"decoder_config\": decoder_config_path,\n",
    "            \"prior\": prior_path,\n",
    "            \"prior_config\": prior_config_path\n",
    "        }\n",
    "    save_state()\n",
    "    install_dependencies(current_state)\n",
    "    \n",
    "redownload_button.on_click(force_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZwesxZ3uAlx4"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# I would suggest running on a remote machine https://research.google.com/colaboratory/local-runtimes.html\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "import importlib\n",
    "\n",
    "\n",
    "from dalle2_pytorch import DALLE2, DiffusionPriorNetwork, DiffusionPrior, OpenAIClipAdapter, train_configs\n",
    "from dalle2_pytorch.tokenizer import tokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D9RjtGoAlx5"
   },
   "source": [
    "### Load the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "WjdOURcWAlx5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def conditioned_on_text(config):\n",
    "    try:\n",
    "        return config.decoder.unets[0].cond_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        return config.decoder.condition_on_text_encodings\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "decoder_text_conditioned = False\n",
    "clip_config = None\n",
    "def load_decoder(decoder_state_dict_path, config_file_path):\n",
    "  config = train_configs.TrainDecoderConfig.from_json_path(config_file_path)\n",
    "  global decoder_text_conditioned\n",
    "  decoder_text_conditioned = conditioned_on_text(config)\n",
    "  global clip_config\n",
    "  clip_config = config.decoder.clip\n",
    "  config.decoder.clip = None\n",
    "  print(\"Decoder conditioned on text\", decoder_text_conditioned)\n",
    "  decoder = config.decoder.create().to(device)\n",
    "  decoder_state_dict = torch.load(decoder_state_dict_path, map_location='cpu')\n",
    "  decoder.load_state_dict(decoder_state_dict, strict=False)\n",
    "  del decoder_state_dict\n",
    "  decoder.eval()\n",
    "  return decoder\n",
    "decoder = load_decoder(current_state[\"model_paths\"][\"decoder\"], current_state[\"model_paths\"][\"decoder_config\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ab__xeOeAlx5"
   },
   "source": [
    "### Load the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ZIGSaY2rAlx5"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def load_prior(model_path):\n",
    "  prior_network = DiffusionPriorNetwork(\n",
    "    dim=768,\n",
    "    depth=24,\n",
    "    dim_head=64,\n",
    "    heads=32,\n",
    "    normformer=True,\n",
    "    attn_dropout=5e-2,\n",
    "    ff_dropout=5e-2,\n",
    "    num_time_embeds=1,\n",
    "    num_image_embeds=1,\n",
    "    num_text_embeds=1,\n",
    "    num_timesteps=1000,\n",
    "    ff_mult=4\n",
    "  )\n",
    "\n",
    "  diffusion_prior = DiffusionPrior(\n",
    "    net=prior_network,\n",
    "    clip=OpenAIClipAdapter(\"ViT-L/14\"),\n",
    "    image_embed_dim=768,\n",
    "    timesteps=1000,\n",
    "    cond_drop_prob=0.1,\n",
    "    loss_type=\"l2\",\n",
    "    condition_on_text_encodings=True,\n",
    "  ).to(device)\n",
    "\n",
    "  state_dict = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "  if 'ema_model' in state_dict:\n",
    "    print('Loading EMA Model')\n",
    "    diffusion_prior.load_state_dict(state_dict['ema_model'], strict=True)\n",
    "  else:\n",
    "    print('Loading Standard Model')\n",
    "    diffusion_prior.load_state_dict(state_dict['model'], strict=False)\n",
    "  del state_dict\n",
    "  return diffusion_prior\n",
    "diffusion_prior = load_prior(current_state[\"model_paths\"][\"prior\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxnLvVQxAlx5"
   },
   "source": [
    "### Load Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ikdfRY9yAlx6"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "clip = None\n",
    "if clip_config is not None:\n",
    "  clip = clip_config.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSMszwrAlx6"
   },
   "source": [
    "### Add helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "sLOGTt6MAlx6"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "class ImageText(object):\n",
    "    def __init__(self, filename_or_size, mode='RGBA', background=(0, 0, 0, 0), encoding='utf8'):\n",
    "        if isinstance(filename_or_size, str):\n",
    "            self.filename = filename_or_size\n",
    "            self.image = Image.open(self.filename)\n",
    "            self.size = self.image.size\n",
    "        elif isinstance(filename_or_size, (list, tuple)):\n",
    "            self.size = filename_or_size\n",
    "            self.image = Image.new(mode, self.size, color=background)\n",
    "            self.filename = None\n",
    "        self.draw = ImageDraw.Draw(self.image)\n",
    "        self.encoding = encoding\n",
    "\n",
    "    def save(self, filename=None):\n",
    "        self.image.save(filename or self.filename)\n",
    "\n",
    "    def get_font_size(self, text, font, max_width=None, max_height=None):\n",
    "        if max_width is None and max_height is None:\n",
    "            raise ValueError('You need to pass max_width or max_height')\n",
    "        font_size = 1\n",
    "        text_size = self.get_text_size(font, font_size, text)\n",
    "        if (max_width is not None and text_size[0] > max_width) or \\\n",
    "           (max_height is not None and text_size[1] > max_height):\n",
    "            raise ValueError(\"Text can't be filled in only (%dpx, %dpx)\" % \\\n",
    "                    text_size)\n",
    "        while True:\n",
    "            if (max_width is not None and text_size[0] >= max_width) or \\\n",
    "               (max_height is not None and text_size[1] >= max_height):\n",
    "                return font_size - 1\n",
    "            font_size += 1\n",
    "            text_size = self.get_text_size(font, font_size, text)\n",
    "\n",
    "    def write_text(self, xy, text, font_filename, font_size=11,\n",
    "                   color=(0, 0, 0), max_width=None, max_height=None):\n",
    "        x, y = xy\n",
    "        # if isinstance(text, str):\n",
    "        #     text = text.decode(self.encoding)\n",
    "        if font_size == 'fill' and \\\n",
    "           (max_width is not None or max_height is not None):\n",
    "            font_size = self.get_font_size(text, font_filename, max_width,\n",
    "                                           max_height)\n",
    "        text_size = self.get_text_size(font_filename, font_size, text)\n",
    "        font = ImageFont.truetype(font_filename, font_size)\n",
    "        # font = ImageFont.load_default()\n",
    "        if x == 'center':\n",
    "            x = (self.size[0] - text_size[0]) / 2\n",
    "        if y == 'center':\n",
    "            y = (self.size[1] - text_size[1]) / 2\n",
    "        self.draw.text((x, y), text, font=font, fill=color)\n",
    "        return text_size\n",
    "\n",
    "    def get_text_size(self, font_filename, font_size, text):\n",
    "        font = ImageFont.truetype(font_filename, font_size)\n",
    "        return font.getsize(text)\n",
    "\n",
    "    def write_text_box(self, xy, text, box_width, font_filename,\n",
    "                       font_size=11, color=(0, 0, 0), place='left',\n",
    "                       justify_last_line=False):\n",
    "        x, y = xy\n",
    "        lines = []\n",
    "        line = []\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            new_line = ' '.join(line + [word])\n",
    "            size = self.get_text_size(font_filename, font_size, new_line)\n",
    "            text_height = size[1]\n",
    "            if size[0] <= box_width:\n",
    "                line.append(word)\n",
    "            else:\n",
    "                lines.append(line)\n",
    "                line = [word]\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        lines = [' '.join(line) for line in lines if line]\n",
    "        height = y\n",
    "        for index, line in enumerate(lines):\n",
    "            if place == 'left':\n",
    "                self.write_text((x, height), line, font_filename, font_size,\n",
    "                                color)\n",
    "            elif place == 'right':\n",
    "                total_size = self.get_text_size(font_filename, font_size, line)\n",
    "                x_left = x + box_width - total_size[0]\n",
    "                self.write_text((x_left, height), line, font_filename,\n",
    "                                font_size, color)\n",
    "            elif place == 'center':\n",
    "                total_size = self.get_text_size(font_filename, font_size, line)\n",
    "                x_left = int(x + ((box_width - total_size[0]) / 2))\n",
    "                self.write_text((x_left, height), line, font_filename,\n",
    "                                font_size, color)\n",
    "            elif place == 'justify':\n",
    "                words = line.split()\n",
    "                if (index == len(lines) - 1 and not justify_last_line) or \\\n",
    "                   len(words) == 1:\n",
    "                    self.write_text((x, height), line, font_filename, font_size,\n",
    "                                    color)\n",
    "                    continue\n",
    "                line_without_spaces = ''.join(words)\n",
    "                total_size = self.get_text_size(font_filename, font_size,\n",
    "                                                line_without_spaces)\n",
    "                space_width = (box_width - total_size[0]) / (len(words) - 1.0)\n",
    "                start_x = x\n",
    "                for word in words[:-1]:\n",
    "                    self.write_text((start_x, height), word, font_filename,\n",
    "                                    font_size, color)\n",
    "                    word_size = self.get_text_size(font_filename, font_size,\n",
    "                                                    word)\n",
    "                    start_x += word_size[0] + space_width\n",
    "                last_word_size = self.get_text_size(font_filename, font_size,\n",
    "                                                    words[-1])\n",
    "                last_word_x = x + box_width - last_word_size[0]\n",
    "                self.write_text((last_word_x, height), words[-1], font_filename,\n",
    "                                font_size, color)\n",
    "            height += text_height\n",
    "        return (box_width, height - y)\n",
    "\n",
    "def download_font():\n",
    "  if not os.path.exists(\"./Arial.ttf\"):\n",
    "    !wget https://github.com/matomo-org/travis-scripts/raw/master/fonts/Arial.ttf\n",
    "  return \"./Arial.ttf\"\n",
    "\n",
    "\n",
    "def map_images(np_images, prior_repeat, decoder_repeat, prompts, upscale=4):\n",
    "  # Match the images to their prompts\n",
    "  # Format [{ prompt: STRING, images: [\n",
    "  #  { prior_index: INT, decoder_index: INT, img: NP_ARR[64, 64, 3] }\n",
    "  # ] }]\n",
    "  image_map = {}\n",
    "  curr_index = 0\n",
    "  for prompt in prompts:\n",
    "    for prior_index in range(prior_repeat):\n",
    "      for decoder_index in range(decoder_repeat):\n",
    "        img = np_images[curr_index]\n",
    "        if prompt not in image_map:\n",
    "          image_map[prompt] = []\n",
    "        if isinstance(img, np.ndarray):\n",
    "          image = Image.fromarray(np.uint8(img * 255))\n",
    "          image = image.resize([dim * upscale for dim in image.size])\n",
    "        else:\n",
    "          image = img\n",
    "        image_map[prompt].append({\n",
    "            \"prior_index\": prior_index,\n",
    "            \"decoder_index\": decoder_index,\n",
    "            \"img\": image\n",
    "        })\n",
    "        curr_index += 1\n",
    "  return image_map\n",
    "\n",
    "def format_image_grid(img_array):\n",
    "  example_image = img_array[0][\"img\"]\n",
    "  max_prior_index = max((inst[\"prior_index\"] for inst in img_array))\n",
    "  cols = max_prior_index + 1\n",
    "  max_decoder_index = max((inst[\"decoder_index\"] for inst in img_array))\n",
    "  rows = max_decoder_index + 1\n",
    "\n",
    "  w, h = example_image.size\n",
    "  grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "  grid_w, grid_h = grid.size\n",
    "  \n",
    "  for img_data in img_array:\n",
    "    x_pos = img_data[\"prior_index\"] * w\n",
    "    y_pos = img_data[\"decoder_index\"] * h\n",
    "    grid.paste(img_data[\"img\"], box=(x_pos, y_pos))\n",
    "  return grid\n",
    "\n",
    "def format_prompt_image(prompt, grid_img, font_size=20, horizontal_padding=10, vertical_padding=10):\n",
    "  grid_w, grid_h = grid_img.size\n",
    "  prompt_img = ImageText((grid_w, 2000), background=(255, 255, 255, 255))\n",
    "  # font = ImageFont.load(\"arial.pil\")\n",
    "  # font = ImageFont.load_default()\n",
    "  font_path = download_font()\n",
    "  text_w, text_h = prompt_img.write_text_box((horizontal_padding, vertical_padding), prompt, box_width=grid_w - horizontal_padding, font_filename=font_path, font_size=font_size, color=(0, 0, 0), place='center')\n",
    "  text_img = prompt_img.image\n",
    "  text_img = text_img.crop((0, 0, grid_w, text_h + 2*vertical_padding))\n",
    "  full_img = Image.new('RGB', (grid_w, text_img.size[1] + grid_h))\n",
    "  full_img.paste(text_img, (0, 0))\n",
    "  full_img.paste(grid_img, (0, text_img.size[1]))\n",
    "  return full_img\n",
    "\n",
    "def save_images(output_dir, np_images):\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  for i, np_img in enumerate(np_images):\n",
    "    image = Image.fromarray(np.uint8(np_img * 255))\n",
    "    output_path = os.path.join(output_dir, f'{i}.png')\n",
    "    image.save(output_path)\n",
    "\n",
    "def upscale_dir(input_dir):\n",
    "  !python SwinIR/main_test_swinir.py --task real_sr --model_path experiments/pretrained_models/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth --folder_lq {input_dir} --scale 4 --large_model\n",
    "  results_dir = \"./results/swinir_real_sr_x4_large\"\n",
    "  output_files = sorted([file for file in os.listdir(results_dir) if '.png' in file], key=lambda e: int(e.split('_')[0]))\n",
    "  upscale_dims = (256, 256, 3)\n",
    "  images = [None] * len(output_files)\n",
    "  for i, filename in enumerate(output_files):\n",
    "    pil_img = Image.open(os.path.join(results_dir, filename))\n",
    "    images[i] = pil_img\n",
    "#   shutil.rmtree(results_dir)\n",
    "  !rm {results_dir}\n",
    "  return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a70fTZfEAlx6"
   },
   "source": [
    "### Start Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SV1GRZxpAlx6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from IPython.display import display, clear_output\n",
    "from ipywidgets import interact\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# colab ì‚¬ìš©ì‹œ í™œì„±í™” \n",
    "# try:\n",
    "#   from google.colab import files\n",
    "#   can_download = True\n",
    "# except ImportError:\n",
    "#   can_download = False\n",
    "\n",
    "can_download = False\n",
    "\n",
    "text_input = widgets.Textarea(\n",
    "    placeholder='Prompts separated by new lines...',\n",
    "    value=str(current_state[\"text_input\"]),\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    layout={'width': 'auto'},\n",
    "    rows=10\n",
    ")\n",
    "\n",
    "file_input = widgets.FileUpload(\n",
    "#     description=\"Image Variation Upload\",\n",
    "    multiple=False\n",
    ")\n",
    "file_input.observe(lambda: print(\"Uploaded\"), names=\"_test\")\n",
    "file_reset = widgets.Button(\n",
    "    description=\"Clear Uploads\"\n",
    ")\n",
    "file_box = widgets.HBox([file_input, file_reset])\n",
    "\n",
    "def reset_file_input(b):\n",
    "    global file_input\n",
    "    new_file_input = widgets.FileUpload(\n",
    "        multiple=False\n",
    "    )\n",
    "    file_input = new_file_input\n",
    "    file_box.children = (new_file_input, file_box.children[1])\n",
    "    render_layout()\n",
    "file_reset.on_click(reset_file_input)\n",
    "\n",
    "textbox_box = widgets.VBox([text_input, file_box], layout={'border': '2px solid grey'})\n",
    "\n",
    "prior_label = widgets.HTML(value=\"<b>Prior Options:</b> Set how many sample to take from the prior and what conditioning scale to use.\")\n",
    "text_repeat = widgets.IntSlider(\n",
    "    value=int(current_state[\"text_repeat\"]),\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Text Repeat',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "\n",
    "prior_conditioning = widgets.FloatSlider(\n",
    "    value=float(current_state[\"prior_conditioning\"]),\n",
    "    min=0.0,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Prior Cond Scale',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "prior_options_box = widgets.VBox([prior_label, text_repeat, prior_conditioning], layout=widgets.Layout(border=\"2px solid grey\", padding=\"5px 10px\", flex=\"1 0 auto\"))\n",
    "\n",
    "decoder_label = widgets.HTML(value=\"<b>Decoder Options:</b> Set how many sample to take from the decoder and what conditioning scale to use.\")\n",
    "img_repeat = widgets.IntSlider(\n",
    "    value=int(current_state[\"img_repeat\"]),\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description='Img Repeat',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "\n",
    "decoder_conditioning = widgets.FloatSlider(\n",
    "    value=float(current_state[\"decoder_conditioning\"]),\n",
    "    min=0.0,\n",
    "    max=10.0,\n",
    "    step=0.1,\n",
    "    description='Decoder Cond Scale',\n",
    "    disabled=False,\n",
    "    continuous_update=True,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width=\"100%\")\n",
    ")\n",
    "decoder_options_box = widgets.VBox([decoder_label, img_repeat, decoder_conditioning], layout=widgets.Layout(border=\"2px solid grey\", padding=\"5px 10px\", flex=\"1 0 auto\"))\n",
    "main_options_box = widgets.HBox([prior_options_box, decoder_options_box], layout=widgets.Layout(width=\"100%\"))\n",
    "\n",
    "include_prompt_checkbox = widgets.Checkbox(\n",
    "    value=bool(current_state[\"include_prompt_checkbox\"]),\n",
    "    description='Show prompt in output image',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "upsample_checkbox = widgets.Checkbox(\n",
    "    value=bool(current_state['upsample_checkbox']),\n",
    "    description = 'Upsample to 256x256 with SwinIR',\n",
    "    disabled=False,\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "meta_options_box = widgets.VBox([include_prompt_checkbox, upsample_checkbox])\n",
    "\n",
    "button = widgets.Button(\n",
    "    description=\"Start\"\n",
    ")\n",
    "final_options_box = widgets.HBox([meta_options_box, button], layout=widgets.Layout(justify_content=\"space-between\", border=\"2px solid grey\", padding=\"10px 30px\"))\n",
    "\n",
    "main_layout = widgets.VBox([textbox_box, main_options_box, final_options_box])\n",
    "\n",
    "def get_prompts():\n",
    "    import json\n",
    "    from itertools import zip_longest\n",
    "    import io\n",
    "    text = text_input.value\n",
    "    try:\n",
    "        prompts_array = json.loads(text)\n",
    "        assert isinstance(prompts_array, list)\n",
    "        text_prompts = prompts_array\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        text_prompts = list(filter(lambda v: len(v) > 0, text.split(\"\\n\")))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to read as text with newlines\", e)\n",
    "        return []\n",
    "\n",
    "    files = file_input.value\n",
    "    file = None\n",
    "    if len(files) > 0:\n",
    "        file_name, file_info = list(files.items())[0]\n",
    "        image_pil = Image.open(io.BytesIO(file_info['content'])).convert('RGB')\n",
    "        transforms = T.Compose([\n",
    "            T.CenterCrop(min(image_pil.size)),\n",
    "            T.Resize(clip.image_size)\n",
    "        ])\n",
    "        image_pil = transforms(image_pil)\n",
    "        file = (file_name, image_pil)\n",
    "    \n",
    "    return (text_prompts, file)\n",
    "\n",
    "def f(text_input, text_repeat, prior_conditioning, img_repeat, decoder_conditioning, include_prompt_checkbox, upsample_checkbox, image):\n",
    "  text_prompts, image_prompt = get_prompts()\n",
    "  total_images = len(text_prompts) * text_repeat * img_repeat\n",
    "\n",
    "  global current_state\n",
    "  current_state = {\n",
    "      **current_state,\n",
    "      \"text_input\": text_input,\n",
    "      \"text_repeat\": text_repeat,\n",
    "      \"prior_conditioning\": prior_conditioning,\n",
    "      \"img_repeat\": img_repeat,\n",
    "      \"decoder_conditioning\": decoder_conditioning,\n",
    "      \"include_prompt_checkbox\": include_prompt_checkbox,\n",
    "      \"upsample_checkbox\": upsample_checkbox,\n",
    "  }\n",
    "    \n",
    "  def get_prompt_text(index):\n",
    "    text_prompt = text_prompts[index]\n",
    "    return f\"Prompt {index}: \\\"{text_prompt}\\\"\"\n",
    "\n",
    "  if image_prompt is not None:\n",
    "    print(\"Taking variation of image\")\n",
    "    display(image_prompt[1])\n",
    "  output_strings = []\n",
    "  output_strings.append(f\"Using model: {current_state['decoder']['name']}\")\n",
    "  output_strings.append(f\"Total output images: {total_images}\")\n",
    "  output_strings.append(\"\")\n",
    "  output_strings.extend([get_prompt_text(index) for index in range(len(text_prompts))])\n",
    "  output_strings.append(\"\")\n",
    "  output_strings.append(\"Including prompt text in output image\" if include_prompt_checkbox else \"Not including prompt text in output image\")\n",
    "  output_strings.append(f\"Prior Conditioning Scale: {prior_conditioning}\")\n",
    "  output_strings.append(f\"Decoder Conditioning Scale: {decoder_conditioning}\")\n",
    "  print('\\n'.join(output_strings))\n",
    "  save_state()\n",
    "\n",
    "def render_layout():\n",
    "    clear_output()\n",
    "    out = widgets.interactive_output(f, {'text_input': text_input, 'text_repeat': text_repeat, 'prior_conditioning': prior_conditioning, 'img_repeat': img_repeat, 'decoder_conditioning': decoder_conditioning, 'include_prompt_checkbox': include_prompt_checkbox, 'upsample_checkbox': upsample_checkbox, 'image': file_input })\n",
    "    display(main_layout, out)\n",
    "    \n",
    "def get_image_embeddings(prompt_tokens, prompt_image, text_rep: int, prior_cond_scale: float):\n",
    "    if prompt_image is None:\n",
    "        print(\"Computing embedings using prior\")\n",
    "        with torch.no_grad():\n",
    "            image_embed = diffusion_prior.sample(prompt_tokens, cond_scale = prior_cond_scale).cpu().numpy()\n",
    "    else:\n",
    "        print(\"Computing embeddings from example image\")\n",
    "        image_tensor = T.ToTensor()(prompt_image[1]).unsqueeze_(0).to(device)\n",
    "        unbatched_image_embed, _ = clip.embed_image(image_tensor)\n",
    "        image_embed = torch.zeros(len(prompt_tokens), unbatched_image_embed.shape[-1])\n",
    "        for i in range(len(prompt_tokens)):\n",
    "            image_embed[i] = unbatched_image_embed\n",
    "        image_embed = image_embed.cpu().numpy()\n",
    "    return image_embed\n",
    "\n",
    "def on_start(_, recall_embeddings=False, recall_images=False):\n",
    "  if os.path.exists(\"./output\"):\n",
    "    shutil.rmtree(\"./output\")\n",
    "  render_layout()\n",
    "  prompts, prompt_image = get_prompts()\n",
    "  prior_cond_scale = prior_conditioning.value\n",
    "  decoder_cond_scale = decoder_conditioning.value\n",
    "  text_rep = text_repeat.value\n",
    "  img_rep = img_repeat.value\n",
    "  include_prompt = include_prompt_checkbox.value\n",
    "  upsample = upsample_checkbox.value\n",
    "  \n",
    "  prior_text_input = []\n",
    "  for prompt in prompts:\n",
    "    for _ in range(text_rep):\n",
    "      prior_text_input.append(prompt)\n",
    "  \n",
    "  tokens = tokenizer.tokenize(prior_text_input).to(device)\n",
    "  if recall_embeddings:\n",
    "    print(\"Loading embeddings\")\n",
    "    image_embed = np.load('img_emb_prior.npy')\n",
    "  else:\n",
    "    image_embed = get_image_embeddings(tokens, prompt_image, text_rep, prior_cond_scale)\n",
    "    np.save('img_emb_prior.npy', image_embed)\n",
    "\n",
    "  embeddings = np.repeat(image_embed, img_rep, axis=0)\n",
    "  embeddings = torch.from_numpy(embeddings).float().to(device)\n",
    "  if recall_images:\n",
    "    print(\"Loading images\")\n",
    "    images = np.load('images_decoder.npy')\n",
    "  else:\n",
    "    print(\"Running decoder\")\n",
    "    with torch.no_grad():\n",
    "      if decoder_text_conditioned:\n",
    "        print(\"Generating clip embeddings\")\n",
    "        _, text_encoding, text_mask = clip.embed_text(tokens)\n",
    "        images = decoder.sample(embeddings, text_encodings = text_encoding, text_mask = text_mask, cond_scale = decoder_cond_scale)\n",
    "      else:\n",
    "        print(\"Not generating clip embeddings\")\n",
    "        images = decoder.sample(embeddings, text = None, cond_scale = decoder_cond_scale)\n",
    "    images = images.cpu().permute(0, 2, 3, 1).numpy()\n",
    "    np.save('images_decoder.npy', images)\n",
    "\n",
    "  if upsample:\n",
    "    save_images('output/images', images)\n",
    "    images = upscale_dir('output/images')\n",
    "\n",
    "  img_map = map_images(images, text_rep, img_rep, prompts, upscale=4)\n",
    "  for index, (prompt, imgs) in enumerate(img_map.items()):\n",
    "    img = format_image_grid(imgs)\n",
    "    if include_prompt:\n",
    "      img = format_prompt_image(prompt, img, font_size=20, horizontal_padding=5, vertical_padding=5)\n",
    "    display(img)\n",
    "    if can_download:\n",
    "      download_button = widgets.Button(\n",
    "        description=\"Download\"\n",
    "      )\n",
    "      display(download_button)\n",
    "      download_button.on_click(lambda b: files.download(f\"./output/example_{index}.png\"))\n",
    "    os.makedirs(\"./output\", exist_ok=True)\n",
    "    img.save(f\"./output/example_{index}.png\")\n",
    "\n",
    "button.on_click(on_start)\n",
    "render_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1worYBzsAlx7"
   },
   "source": [
    "### Rerank Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "pG8ErLZ6Alx7"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\"\"\"\n",
    "Here we take the prompt, generate n number of embeddings and rerank them by cosine similarity to the text embedding, \n",
    "then take a linspace of N and sample the decoder with those embeddings to see the variation in the performance of the prior\n",
    "\"\"\"\n",
    "def rerank_and_sample(image_embeddings, text_embedding, samples=None, strategy=\"top\"):\n",
    "  if samples is None:\n",
    "    samples = len(image_embeddings)\n",
    "  \n",
    "  def similarity(image_embedding, text_embedding):\n",
    "    image_embedding = image_embedding / np.linalg.norm(image_embedding)\n",
    "    text_embedding = text_embedding / np.linalg.norm(text_embedding)\n",
    "    return np.inner(image_embedding, text_embedding)\n",
    "\n",
    "  reranked = sorted(list(image_embeddings), key=lambda img_emb: similarity(img_emb, text_embedding))\n",
    "  if strategy == \"top\":\n",
    "    sampled_embeddings = np.array(reranked[-samples:])\n",
    "  elif strategy == \"even\":\n",
    "    sample_indices = np.linspace(0, len(reranked) - 1, num=samples, dtype=int)\n",
    "    sampled_embeddings = np.array([reranked[i] for i in sample_indices])\n",
    "  rankings = [similarity(emb, text_embedding) for emb in sampled_embeddings]\n",
    "  print(rankings, rankings[0], rankings[-1])\n",
    "  return sampled_embeddings\n",
    "\n",
    "def rerank_test(num_samples, num_trials):\n",
    "  prompts = get_prompts()\n",
    "  prior_cond_scale = prior_conditioning.value\n",
    "  decoder_cond_scale = decoder_conditioning.value\n",
    "  text_rep = text_repeat.value\n",
    "  img_rep = img_repeat.value\n",
    "  include_prompt = include_prompt_checkbox.value\n",
    "\n",
    "  prompt_tokens = tokenizer.tokenize(prompts).to(device)\n",
    "  prompt_embeddings, prompt_encoding, prompt_mask = clip.embed_text(prompt_tokens)\n",
    "  prompt_embeddings = prompt_embeddings.cpu().numpy()\n",
    "\n",
    "  prior_prompts = []\n",
    "  for prompt in prompts:\n",
    "    for _ in range(num_trials):\n",
    "      prior_prompts.append(prompt)\n",
    "\n",
    "  sample_prompts = []\n",
    "  for prompt in prompts:\n",
    "    for _ in range(num_samples):\n",
    "      sample_prompts.append(prompt)\n",
    "\n",
    "  tokens = tokenizer.tokenize(prior_prompts).to(device)\n",
    "  with torch.no_grad():\n",
    "      image_embed = diffusion_prior.sample(tokens, cond_scale = prior_cond_scale).cpu().numpy()\n",
    "  print(f\"Generated {len(image_embed)} image embeddings\")\n",
    "\n",
    "  image_embed = np.split(image_embed, len(prompts))\n",
    "  reranked_embeddings = []\n",
    "  for i, embedding_set in enumerate(image_embed):\n",
    "    reranked_embeddings.append(rerank_and_sample(embedding_set, prompt_embeddings[i], samples=num_samples))\n",
    "  \n",
    "  sampled_embedding_array = np.concatenate(reranked_embeddings)\n",
    "  print(f\"After reranking there are {len(sampled_embedding_array)} image embeddings\")\n",
    "  sampled_embedding_tensor = torch.from_numpy(sampled_embedding_array).to(device)\n",
    "\n",
    "  sample_tokens = tokenizer.tokenize(sample_prompts).to(device)\n",
    "  with torch.no_grad():\n",
    "    if decoder_text_conditioned:\n",
    "      print(\"Generating clip embeddings\")\n",
    "      _, text_encoding, text_mask = clip.embed_text(sample_tokens)\n",
    "      images = decoder.sample(sampled_embedding_tensor, text_encodings = text_encoding, text_mask = text_mask, cond_scale = decoder_cond_scale)\n",
    "    else:\n",
    "      print(\"Not generating clip embeddings\")\n",
    "      images = decoder.sample(sampled_embedding_tensor, text = None, cond_scale = decoder_cond_scale)\n",
    "  np_images = images.cpu().permute(0, 2, 3, 1)\n",
    "  img_map = map_images(np_images, num_samples, 1, prompts, upscale=4)\n",
    "  for index, (prompt, imgs) in enumerate(img_map.items()):\n",
    "    img = format_image_grid(imgs)\n",
    "    if include_prompt:\n",
    "      img = format_prompt_image(prompt, img, font_size=20, horizontal_padding=5, vertical_padding=5)\n",
    "    display(img)\n",
    "    if can_download:\n",
    "      download_button = widgets.Button(\n",
    "        description=\"Download\"\n",
    "      )\n",
    "      display(download_button)\n",
    "      download_button.on_click(lambda b: files.download(f\"./output/example_{index}.png\"))\n",
    "    os.makedirs(\"./reranked_output\", exist_ok=True)\n",
    "    img.save(f\"./reranked_output/example_{index}.png\")\n",
    "\n",
    "rerank_test(5, 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DALLE2-Inference-Demo.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python-dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
